import os, subprocess, json, threading, time, socket, datetime, uuid, csv, re, gzip, copy
import requests, urllib3, psutil
from flask import Flask, render_template, request, jsonify, send_from_directory, make_response, redirect
from urllib.parse import urlparse
from apscheduler.schedulers.background import BackgroundScheduler
from concurrent.futures import ThreadPoolExecutor
import xml.etree.ElementTree as ET
from io import BytesIO
from sqlalchemy import create_engine, Column, String, Integer, Float, DateTime, Text, Boolean, JSON, and_
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.pool import StaticPool

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
app = Flask(__name__)

# --- æ•°æ®åº“é…ç½® ---
DATA_DIR = "/app/data"
LOG_DIR = os.path.join(DATA_DIR, "log")
OUTPUT_DIR = os.path.join(DATA_DIR, "output")
CONFIG_FILE = os.path.join(DATA_DIR, "config.json")  # æ—§æ–‡ä»¶ï¼Œä»…è¿ç§»ä½¿ç”¨
ALIAS_FILE = os.path.join(DATA_DIR, "alias.txt")
DEMO_FILE = os.path.join(DATA_DIR, "demo.txt")
PENDING_FILE = os.path.join(DATA_DIR, "pending.json")  # æ—§æ–‡ä»¶ï¼Œä»…è¿ç§»ä½¿ç”¨
EPG_CACHE_DIR = os.path.join(DATA_DIR, "epg_cache")
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(EPG_CACHE_DIR, exist_ok=True)

DB_PATH = os.path.join(DATA_DIR, "m3u_checker.db")
engine = create_engine(f'sqlite:///{DB_PATH}?check_same_thread=False', poolclass=StaticPool)
db_session = scoped_session(sessionmaker(bind=engine))
Base = declarative_base()

# --- å®šä¹‰æ¨¡å‹ ---
class Subscription(Base):
    __tablename__ = 'subscriptions'
    id = Column(String(50), primary_key=True)
    name = Column(String(200), nullable=False)
    url = Column(Text, nullable=False)
    threads = Column(Integer, default=10)
    enabled = Column(Boolean, default=True)
    schedule_mode = Column(String(20), default='none')
    fixed_times = Column(String(500), default='')
    interval_hours = Column(Integer, default=1)
    res_filter = Column(JSON, default=['sd','720p','1080p','4k','8k'])
    created_at = Column(DateTime, default=datetime.datetime.now)

class Aggregate(Base):
    __tablename__ = 'aggregates'
    id = Column(String(50), primary_key=True)
    name = Column(String(200), nullable=False)
    subscription_ids = Column(JSON)
    strategy = Column(String(20), default='best_score')
    enabled = Column(Boolean, default=True)
    epg_aggregate_id = Column(String(50), nullable=True)
    last_update = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.now)

class EPGAggregate(Base):
    __tablename__ = 'epg_aggregates'
    id = Column(String(50), primary_key=True)
    name = Column(String(200), nullable=False)
    sources = Column(JSON)
    cache_days = Column(Integer, default=3)
    update_interval = Column(Integer, default=24)
    enabled = Column(Boolean, default=True)
    last_update = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.now)

class Setting(Base):
    __tablename__ = 'settings'
    key = Column(String(100), primary_key=True)
    value = Column(Text, nullable=False)

class ProbeResult(Base):
    __tablename__ = 'probe_results'
    id = Column(Integer, primary_key=True, autoincrement=True)
    sub_id = Column(String(50), nullable=False, index=True)
    channel_name = Column(String(500), nullable=False)
    url = Column(Text, nullable=False)
    score = Column(Float, default=0)
    res_tag = Column(String(20))
    probe_time = Column(DateTime, default=datetime.datetime.now, index=True)

class PendingChannel(Base):
    __tablename__ = 'pending_channels'
    id = Column(Integer, primary_key=True, autoincrement=True)
    raw_name = Column(String(500), unique=True, nullable=False)
    count = Column(Integer, default=1)
    first_seen = Column(DateTime, default=datetime.datetime.now)
    sub_ids = Column(JSON)

# åˆ›å»ºè¡¨
Base.metadata.create_all(bind=engine)

# ---------- æ—§æ•°æ®è¿ç§»è¾…åŠ©å‡½æ•°ï¼ˆå·²åˆ é™¤JSONæ–‡ä»¶ï¼Œä¿ç•™ä½†ä¸å†ä½¿ç”¨ï¼‰----------
def migrate_from_json():
    pass  # å·²æ‰‹åŠ¨åˆ é™¤JSONæ–‡ä»¶ï¼Œæ— éœ€è¿ç§»

# ---------- åˆ«ååŠ è½½ä¸åŒ¹é… ----------
ALIAS_CACHE = None
ALIAS_MTIME = None

def load_aliases():
    global ALIAS_CACHE, ALIAS_MTIME
    if not os.path.exists(ALIAS_FILE):
        return {}
    mtime = os.path.getmtime(ALIAS_FILE)
    if ALIAS_CACHE is not None and ALIAS_MTIME == mtime:
        return ALIAS_CACHE
    aliases = {}
    with open(ALIAS_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split(',')
            main_name = parts[0].strip()
            alias_list = [a.strip() for a in parts[1:]]
            compiled = []
            for a in alias_list:
                if a.startswith('re:'):
                    try:
                        compiled.append(('re', re.compile(a[3:], re.IGNORECASE)))
                    except:
                        continue
                else:
                    compiled.append(('plain', a.lower()))
            aliases[main_name] = compiled
    ALIAS_CACHE = aliases
    ALIAS_MTIME = mtime
    return aliases

def match_channel_name(raw_name):
    aliases = load_aliases()
    raw_lower = raw_name.lower()
    for main_name, patterns in aliases.items():
        for ptype, p in patterns:
            if ptype == 'plain':
                if p in raw_lower:
                    return main_name, True
            else:
                if p.search(raw_name):
                    return main_name, True
    return raw_name, False

# ---------- å·¥å…·å‡½æ•° ----------
def get_now():
    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def get_today():
    return datetime.datetime.now().strftime('%Y-%m-%d')

def format_duration(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

def load_config():
    """ä»æ•°æ®åº“åŠ è½½é…ç½®ï¼Œå§‹ç»ˆè¿”å›åŒ…å«é»˜è®¤ settings çš„å­—å…¸"""
    config = {
        "subscriptions": [],
        "aggregates": [],
        "epg_aggregates": [],
        "settings": {
            "use_hwaccel": True,
            "epg_url": "http://epg.51zmt.top:12489/e.xml",
            "logo_base": "https://live.fanmingming.com/tv/"
        }
    }
    with db_session() as session:
        for sub in session.query(Subscription).all():
            config["subscriptions"].append({
                "id": sub.id,
                "name": sub.name,
                "url": sub.url,
                "threads": sub.threads,
                "enabled": sub.enabled,
                "schedule_mode": sub.schedule_mode,
                "fixed_times": sub.fixed_times,
                "interval_hours": sub.interval_hours,
                "res_filter": sub.res_filter
            })
        for agg in session.query(Aggregate).all():
            config["aggregates"].append({
                "id": agg.id,
                "name": agg.name,
                "subscription_ids": agg.subscription_ids,
                "strategy": agg.strategy,
                "enabled": agg.enabled,
                "epg_aggregate_id": agg.epg_aggregate_id
            })
        for epg in session.query(EPGAggregate).all():
            config["epg_aggregates"].append({
                "id": epg.id,
                "name": epg.name,
                "sources": epg.sources,
                "cache_days": epg.cache_days,
                "update_interval": epg.update_interval,
                "enabled": epg.enabled
            })
        for setting in session.query(Setting).all():
            config["settings"][setting.key] = setting.value
    return config

def save_config(config):
    """ä¿å­˜é…ç½®åˆ°æ•°æ®åº“"""
    with db_session() as session:
        # æ›´æ–° subscriptions
        for sub_data in config["subscriptions"]:
            sub = session.get(Subscription, sub_data['id'])
            if sub:
                sub.name = sub_data['name']
                sub.url = sub_data['url']
                sub.threads = sub_data.get('threads', 10)
                sub.enabled = sub_data.get('enabled', True)
                sub.schedule_mode = sub_data.get('schedule_mode', 'none')
                sub.fixed_times = sub_data.get('fixed_times', '')
                sub.interval_hours = sub_data.get('interval_hours', 1)
                sub.res_filter = sub_data.get('res_filter', ['sd','720p','1080p','4k','8k'])
            else:
                session.add(Subscription(**sub_data))
        
        # æ›´æ–° aggregates
        for agg_data in config["aggregates"]:
            agg = session.get(Aggregate, agg_data['id'])
            if agg:
                agg.name = agg_data['name']
                agg.subscription_ids = agg_data.get('subscription_ids', [])
                agg.strategy = agg_data.get('strategy', 'best_score')
                agg.enabled = agg_data.get('enabled', True)
                agg.epg_aggregate_id = agg_data.get('epg_aggregate_id')
            else:
                session.add(Aggregate(**agg_data))
        
        # æ›´æ–° epg_aggregates
        for epg_data in config["epg_aggregates"]:
            epg = session.get(EPGAggregate, epg_data['id'])
            if epg:
                epg.name = epg_data['name']
                epg.sources = epg_data.get('sources', [])
                epg.cache_days = epg_data.get('cache_days', 3)
                epg.update_interval = epg_data.get('update_interval', 24)
                epg.enabled = epg_data.get('enabled', True)
            else:
                session.add(EPGAggregate(**epg_data))
        
        # æ›´æ–° settings
        for key, value in config["settings"].items():
            setting = session.get(Setting, key)
            if setting:
                setting.value = str(value)
            else:
                session.add(Setting(key=key, value=str(value)))
        session.commit()
    reschedule_all()
    reschedule_epg_all()

# ---------- CSV æ—¥å¿—è®°å½• ----------
def write_log_csv(row_dict):
    csv_path = os.path.join(LOG_DIR, f"{get_today()}.csv")
    file_exists = os.path.isfile(csv_path)
    with file_lock:
        with open(csv_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=row_dict.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(row_dict)

# ---------- å…¨å±€çŠ¶æ€ ----------
subs_status, ip_cache = {}, {}
aggregates_status = {}
epg_aggregates_status = {}
api_lock, log_lock, file_lock = threading.Lock(), threading.Lock(), threading.Lock()
scheduler = BackgroundScheduler()
scheduler.start()

# ---------- åœ°ç†å®šä½ï¼ˆæ‰¹é‡ç‰ˆï¼‰----------
def fetch_ip_locations_sync(sub_id, host_list):
    status = subs_status[sub_id]
    total = len(host_list)
    status["logs"].append(f"ğŸŒ é˜¶æ®µ 1/2: æ­£åœ¨æ£€ç´¢ {total} ä¸ªèŠ‚ç‚¹çš„åœ°ç†ä½ç½®...")

    ips_to_query = []
    ip_to_host = {}
    for host in host_list:
        if host in ip_cache:
            continue
        try:
            ip = socket.gethostbyname(host)
            if ip in ip_cache:
                ip_cache[host] = ip_cache[ip]
                continue
            ips_to_query.append(ip)
            ip_to_host[ip] = host
        except:
            pass

    ips_to_query = list(set(ips_to_query))
    if not ips_to_query:
        status["logs"].append("âœ… é˜¶æ®µ 1/2: æ‰€æœ‰èŠ‚ç‚¹å‡å·²ç¼“å­˜ï¼Œæ— éœ€æŸ¥è¯¢ã€‚")
        return

    batch_size = 100
    total_ips = len(ips_to_query)
    queried = 0
    for i in range(0, total_ips, batch_size):
        if status.get("stop_requested"):
            break
        batch = ips_to_query[i:i+batch_size]
        try:
            with api_lock:
                time.sleep(1.35)
                r = requests.post(
                    "http://ip-api.com/batch",
                    json=batch,
                    timeout=10,
                    verify=False
                ).json()
            for idx, info in enumerate(r):
                ip = batch[idx]
                if info.get('status') == 'success':
                    city = info.get('city', 'æœªçŸ¥')
                    isp = info.get('isp', 'æœªçŸ¥')
                    ip_cache[ip] = {"city": city, "isp": isp}
                    host = ip_to_host.get(ip)
                    if host:
                        ip_cache[host] = ip_cache[ip]
                        status["logs"].append(f"ğŸ“ å®šä½åˆ†æ [{queried+idx+1}/{total_ips}]: {host} -> {city}")
                else:
                    ip_cache[ip] = {"city": "æœªçŸ¥", "isp": "æœªçŸ¥"}
            queried += len(batch)
        except Exception as e:
            status["logs"].append(f"âš ï¸ æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
            for ip in batch:
                ip_cache[ip] = {"city": "æœªçŸ¥", "isp": "æœªçŸ¥"}
            queried += len(batch)

    status["logs"].append(f"âœ… é˜¶æ®µ 1/2: å®šä½é¢„æ£€å·²å®Œæˆã€‚")

# ---------- FFprobe æ¢æµ‹ï¼ˆå¸¦è°ƒè¯•ï¼‰----------
def probe_stream(url, use_hw):
    accel_type = os.getenv("HW_ACCEL_TYPE", "vaapi").lower()
    device = os.getenv("VAAPI_DEVICE") or os.getenv("QSV_DEVICE") or "/dev/dri/renderD128"
    
    def run_f(hw, icon, mode_name):
        cmd = ['ffprobe', '-v', 'error', '-show_format', '-show_streams', '-print_format', 'json',
               '-user_agent', 'Mozilla/5.0', '-probesize', '5000000', '-analyzeduration', '5000000'] + hw + ['-i', url]
        try:
            r = subprocess.run(cmd, capture_output=True, text=True, timeout=12)
            if r.returncode == 0:
                data = json.loads(r.stdout)
                streams = data.get('streams', [])
                v = next((s for s in streams if s['codec_type'] == 'video'), {})
                a = next((s for s in streams if s['codec_type'] == 'audio'), {})
                fmt = data.get('format', {})
                rb = fmt.get('bit_rate') or v.get('bit_rate') or "0"
                fps = "?"
                afps = v.get('avg_frame_rate', '0/0')
                if '/' in afps:
                    num, den = afps.split('/')
                    if int(den) > 0:
                        fps = str(round(int(num)/int(den)))
                if os.getenv('DEBUG_HW') == '1':
                    print(f"[HW] {mode_name} succeeded for {url}")
                return {
                    "res": f"{v.get('width','?')}x{v.get('height','?')}",
                    "h": v.get('height', 0),
                    "v_codec": v.get('codec_name', 'UNK').upper(),
                    "a_codec": a.get('codec_name', 'UNK').upper() if a else "æ— éŸ³é¢‘",
                    "fps": fps,
                    "br": f"{round(int(rb)/1024/1024, 2)}Mbps",
                    "icon": icon
                }
        except Exception as e:
            if os.getenv('DEBUG_HW') == '1':
                print(f"[HW] {mode_name} failed: {e}")
        return None
    
    if use_hw:
        hw_p = ['-hwaccel', 'vaapi', '-hwaccel_device', device, '-hwaccel_output_format', 'vaapi'] if accel_type == "vaapi" else ['-hwaccel', 'qsv', '-qsv_device', device]
        res = run_f(hw_p, "ğŸ’", "vaapi/qsv")
        if res:
            return res
        if os.getenv('DEBUG_HW') == '1':
            print(f"Hardware acceleration failed for {url}, falling back to software")
    return run_f([], "ğŸ’»", "software")

# ---------- å¾…å¤„ç†é¢‘é“ç®¡ç† ----------
def add_pending_channel(raw_name, sub_id):
    with db_session() as session:
        pc = session.query(PendingChannel).filter_by(raw_name=raw_name).first()
        if pc:
            pc.count += 1
            if sub_id not in pc.sub_ids:
                sub_ids = pc.sub_ids or []
                sub_ids.append(sub_id)
                pc.sub_ids = sub_ids
        else:
            pc = PendingChannel(raw_name=raw_name, count=1, sub_ids=[sub_id])
            session.add(pc)
        session.commit()

def append_alias(main_name, aliases):
    with open(ALIAS_FILE, 'a', encoding='utf-8') as f:
        line = f"{main_name}," + ",".join(aliases) + "\n"
        f.write(line)
    global ALIAS_CACHE, ALIAS_MTIME
    ALIAS_CACHE = None
    ALIAS_MTIME = None

def append_to_demo(channel_name, group_name):
    with open(DEMO_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{channel_name}\n")

# ---------- å•é¢‘é“æµ‹è¯• ----------
def test_single_channel(sub_id, name, url, use_hw):
    status = subs_status[sub_id]

    if status.get("stop_requested"):
        with log_lock:
            status["current"] += 1
        return None

    parsed = urlparse(url)
    host = parsed.hostname
    hp = f"{host}:{parsed.port or (443 if parsed.scheme=='https' else 80)}"

    if hp in status["blacklisted_hosts"]:
        with log_lock:
            status["analytics"]["stability"]["banned"] += 1
            status["current"] += 1
        return None

    with log_lock:
        if hp not in status["summary_host"]:
            status["summary_host"][hp] = {"t": 0, "s": 0, "f": 0, "lat_sum": 0, "speed_sum": 0, "score_sum": 0}
        if hp not in status["consecutive_failures"]:
            status["consecutive_failures"][hp] = 0

    geo = None
    try:
        start_time = time.time()
        with requests.get(url, stream=True, timeout=8, verify=False,
                          headers={'User-Agent': 'Mozilla/5.0'}) as resp:
            if resp.status_code != 200:
                raise Exception(f"HTTP {resp.status_code}")
            latency = int((time.time() - start_time) * 1000)
            td, ss = 0, time.time()
            for chunk in resp.iter_content(chunk_size=128*1024):
                if status.get("stop_requested"):
                    return None
                td += len(chunk)
                if time.time() - ss > 2:
                    break
            speed = round((td * 8) / ((time.time() - ss) * 1024 * 1024), 2)

        meta = probe_stream(url, use_hw)
        if not meta:
            raise Exception("ProbeFail")

        geo = ip_cache.get(host) or {"city": "æœªçŸ¥", "isp": "æœªçŸ¥"}

        with log_lock:
            status["consecutive_failures"][hp] = 0
            status["success"] += 1
            status["summary_host"][hp]["s"] += 1
            if geo['city'] not in status["summary_city"]:
                status["summary_city"][geo['city']] = {"t": 0, "s": 0}
            status["summary_city"][geo['city']]["s"] += 1
            status["summary_host"][hp]["lat_sum"] += latency
            status["summary_host"][hp]["speed_sum"] += speed
            h = int(meta['h'])
            res_tag = "8K" if h >= 4320 else "4K" if h >= 2160 else "1080P" if h >= 1080 else "720P" if h >= 720 else "SD"
            status["analytics"]["res"][res_tag] += 1
            latency_cat = "<100ms" if latency < 100 else "<500ms" if latency < 500 else ">500ms"
            status["analytics"]["lat"][latency_cat] += 1
            status["analytics"]["v_codec"][meta['v_codec']] = status["analytics"]["v_codec"].get(meta['v_codec'], 0) + 1
            status["analytics"]["a_codec"][meta['a_codec']] = status["analytics"]["a_codec"].get(meta['a_codec'], 0) + 1
            status["analytics"]["stability"]["success"] += 1
            isp_name = geo.get('isp', 'æœªçŸ¥')
            status["analytics"]["isp"][isp_name] = status["analytics"]["isp"].get(isp_name, 0) + 1
            protocol = parsed.scheme
            if protocol in ('http', 'https'):
                status["analytics"]["protocol"][protocol] += 1
            br_value = float(meta['br'].replace('Mbps','').strip()) if 'Mbps' in meta['br'] else 0
            if br_value < 1:
                status["analytics"]["bitrate"]["<1M"] += 1
            elif br_value < 5:
                status["analytics"]["bitrate"]["1-5M"] += 1
            elif br_value < 10:
                status["analytics"]["bitrate"]["5-10M"] += 1
            else:
                status["analytics"]["bitrate"][">10M"] += 1

            score = h + speed * 5 - latency / 10
            status["summary_host"][hp]["score_sum"] += score
            fps_display = f"{meta['fps']} fps" if meta['fps'] != "?" else "?"
            msg = (f"âœ… {name}: {meta['icon']}{meta['res']} | ğŸ¬{meta['v_codec']} | ğŸµ{meta['a_codec']} | "
                   f"ğŸï¸{fps_display} | ğŸ“Š{speed}Mbps | â±ï¸{latency}ms | ğŸ“{geo['city']} | ğŸŒ{hp}")
            status["logs"].append(msg)
            write_log_csv({
                "æ—¶é—´": get_now(),
                "ä»»åŠ¡": status['sub_name'],
                "çŠ¶æ€": "æˆåŠŸ",
                "é¢‘é“": name,
                "åˆ†è¾¨ç‡": meta['res'],
                "è§†é¢‘ç¼–ç ": meta['v_codec'],
                "éŸ³é¢‘ç¼–ç ": meta['a_codec'],
                "FPS": meta['fps'],
                "å»¶è¿Ÿ(ms)": latency,
                "ç½‘é€Ÿ(Mbps)": speed,
                "åœ°åŒº": geo['city'],
                "è¿è¥å•†": geo['isp'],
                "URL": url
            })
        
        # æ£€æŸ¥æ˜¯å¦æœªåŒ¹é…åˆ«åï¼Œè‹¥æ˜¯åˆ™åŠ å…¥å¾…å¤„ç†
        std_name, matched = match_channel_name(name)
        if not matched:
            add_pending_channel(name, sub_id)
        
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        with db_session() as session:
            session.add(ProbeResult(
                sub_id=sub_id,
                channel_name=std_name,
                url=url,
                score=score,
                res_tag=res_tag.lower(),
                probe_time=datetime.datetime.now()
            ))
            session.commit()

        return {"name": name, "url": url, "score": score, "res_tag": res_tag.lower()}
    except Exception as e:
        with log_lock:
            status["consecutive_failures"][hp] += 1
            status["summary_host"][hp]["f"] += 1
            status["analytics"]["stability"]["fail"] += 1
            if status["consecutive_failures"][hp] >= 10:
                if hp not in status["blacklisted_hosts"]:
                    status["blacklisted_hosts"].add(hp)
                    status["logs"].append(f"âš ï¸ ç†”æ–­æ¿€æ´»: æ¥å£ {hp} è¿ç»­å¤±è´¥10æ¬¡ï¼Œå·²è·³è¿‡ã€‚")
            if not status.get("stop_requested"):
                status["logs"].append(f"âŒ {name}: å¤±è´¥({str(e)}) | ğŸŒ{hp}")
        return None
    finally:
        with log_lock:
            status["current"] += 1
            status["summary_host"][hp]["t"] += 1
            city = geo['city'] if geo else "æœªçŸ¥åŸå¸‚"
            if city not in status["summary_city"]:
                status["summary_city"][city] = {"t": 0, "s": 0}
            status["summary_city"][city]["t"] += 1

# ---------- ä»»åŠ¡è¿è¡Œ ----------
def run_task(sub_id):
    # ä»æ•°æ®åº“åŠ è½½è®¢é˜…ä¿¡æ¯
    with db_session() as session:
        sub = session.get(Subscription, sub_id)
        if not sub or not sub.enabled:
            return
        sub_name = sub.name
        sub_url = sub.url
        threads = sub.threads or 10
        res_filter = sub.res_filter or ["sd", "720p", "1080p", "4k", "8k"]
    
    # ä»é…ç½®ä¸­è·å– use_hwï¼ˆå®‰å…¨æ–¹å¼ï¼‰
    config = load_config()
    use_hw = config.get("settings", {}).get("use_hwaccel", True)

    if subs_status.get(sub_id, {}).get("running"):
        return
    start_ts = time.time()
    subs_status[sub_id] = {
        "running": True,
        "stop_requested": False,
        "total": 0,
        "current": 0,
        "success": 0,
        "sub_name": sub_name,
        "logs": [],
        "summary_host": {},
        "summary_city": {},
        "consecutive_failures": {},
        "blacklisted_hosts": set(),
        "analytics": {
            "res": {"SD": 0, "720P": 0, "1080P": 0, "4K": 0, "8K": 0},
            "lat": {"<100ms": 0, "<500ms": 0, ">500ms": 0},
            "v_codec": {},
            "a_codec": {},
            "stability": {"success": 0, "fail": 0, "banned": 0},
            "isp": {},
            "protocol": {"http": 0, "https": 0},
            "bitrate": {"<1M": 0, "1-5M": 0, "5-10M": 0, ">10M": 0}
        }
    }

    # æ‹‰å–è®¢é˜…å†…å®¹
    raw_channels = []
    try:
        r = requests.get(sub_url, timeout=15, verify=False)
        r.encoding = r.apparent_encoding
        content = r.text
        if "#EXTINF" in content:
            last_name = "æœªçŸ¥é¢‘é“"
            for line in content.split('\n'):
                line = line.strip()
                if not line:
                    continue
                if "#EXTINF" in line:
                    last_name = line.split(',')[-1].strip()
                elif "://" in line:
                    raw_channels.append((last_name, line))
        else:
            for line in content.split('\n'):
                if "," in line and "://" in line:
                    p = line.split(',')
                    raw_channels.append((p[0].strip(), p[1].strip()))
    except Exception as e:
        subs_status[sub_id]["logs"].append(f"âŒ è®¢é˜…æ‹‰å–å¤±è´¥: {e}")
        subs_status[sub_id]["running"] = False
        return

    raw_channels = list(set(raw_channels))
    total_num = len(raw_channels)
    subs_status[sub_id]["total"] = total_num

    if total_num > 0:
        unique_hosts = list(set([urlparse(c[1]).hostname for c in raw_channels if c[1]]))
        fetch_ip_locations_sync(sub_id, unique_hosts)

        subs_status[sub_id]["logs"].append(f"ğŸš€ é˜¶æ®µ 2/2: å¼€å§‹æ¢æµ‹ {total_num} ä¸ªé¢‘é“...")

        with ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(test_single_channel, sub_id, n, u, use_hw) for n, u in raw_channels]
            valid_raw = []
            for f in futures:
                if subs_status[sub_id].get("stop_requested"):
                    pass
                try:
                    res = f.result(timeout=30)
                    if res:
                        valid_raw.append(res)
                except Exception as e:
                    subs_status[sub_id]["logs"].append(f"âš ï¸ ä»»åŠ¡å¼‚å¸¸: {str(e)}")
    else:
        valid_raw = []

    valid_list = [c for c in valid_raw if c['res_tag'] in res_filter]
    valid_list.sort(key=lambda x: x['score'], reverse=True)

    status = subs_status[sub_id]
    duration = format_duration(time.time() - start_ts)
    update_ts = get_now()

    # ç”ŸæˆæŠ¥å‘Š
    status["logs"].append(" ")
    status["logs"].append("ğŸ“œ ==================== æ¢æµ‹ç»“ç®—æŠ¥å‘Š ====================")
    status["logs"].append(f"â±ï¸ ä»»åŠ¡æ€»è€—æ—¶: {duration} | æœ‰æ•ˆæº: {len(valid_list)} / æˆåŠŸæ¢æµ‹: {status['success']}")
    status["logs"].append("ğŸ™ï¸ --- åœ°åŒºè¿é€šæ±‡æ€» ---")
    sc = sorted([i for i in status["summary_city"].items() if i[1]['t'] > 0],
                key=lambda x: x[1]['s']/x[1]['t'], reverse=True)
    for c, d in sc:
        status["logs"].append(f"ğŸ“ {c:<30} | æœ‰æ•ˆç‡: {round(d['s']/d['t']*100, 1)}% ({d['s']}/{d['t']})")
    status["logs"].append("ğŸ“¡ --- æ¥å£è´¨é‡å…¨è¡¨ (æŒ‰è¯„åˆ†) ---")
    ah = {k: v for k, v in status["summary_host"].items() if k not in status["blacklisted_hosts"] and v['t'] > 0}
    sh = sorted(ah.items(), key=lambda x: x[1]['score_sum']/x[1]['s'] if x[1]['s'] > 0 else 0, reverse=True)
    for h, d in sh:
        al = int(d['lat_sum']/d['s']) if d['s'] > 0 else 0
        aspd = round(d['speed_sum']/d['s'], 2) if d['s'] > 0 else 0
        status["logs"].append(f"{'â­ï¸' if d['s']/d['t'] > 0.8 else 'ğŸ“¡'} {h:<24} | â±ï¸{al}ms | ğŸš€{aspd}Mbps | æœ‰æ•ˆç‡: {round(d['s']/d['t']*100, 1)}%")
    if status["blacklisted_hosts"]:
        status["logs"].append("ğŸš« --- å·²ç†”æ–­çš„æ¥å£æ¸…å• ---")
        for bh in status["blacklisted_hosts"]:
            status["logs"].append(f"âŒ {bh} (è¿ç»­10æ¬¡å¤±è´¥)")
    status["logs"].append("ğŸ“Š --- è¿è¥å•†åˆ†å¸ƒ ---")
    isp_sorted = sorted(status["analytics"]["isp"].items(), key=lambda x: x[1], reverse=True)[:10]
    for isp, count in isp_sorted:
        status["logs"].append(f"ğŸ“¡ {isp}: {count}")
    status["logs"].append("ğŸŒ --- åè®®æ¯”ä¾‹ ---")
    for proto, count in status["analytics"]["protocol"].items():
        status["logs"].append(f"{proto.upper()}: {count}")
    status["logs"].append("ğŸ“ˆ --- æ¯”ç‰¹ç‡åˆ†æ®µ ---")
    for br_range, count in status["analytics"]["bitrate"].items():
        status["logs"].append(f"{br_range}: {count}")
    status["logs"].append("======================================================")
    status["logs"].append(f"ğŸ ä»»åŠ¡å®Œæˆæ—¶é—´: {get_now()}")

    # è¾“å‡º M3U å’Œ TXT
    try:
        m3u_p = os.path.join(OUTPUT_DIR, f"{sub_id}.m3u")
        txt_p = os.path.join(OUTPUT_DIR, f"{sub_id}.txt")
        epg = config.get("settings", {}).get("epg_url", "")
        logo = config.get("settings", {}).get("logo_base", "")
        with open(m3u_p, 'w', encoding='utf-8') as fm:
            fm.write(f"#EXTM3U x-tvg-url=\"{epg}\"\n# Updated: {update_ts}\n# Duration: {duration}\n")
            for c in valid_list:
                fm.write(f"#EXTINF:-1 tvg-logo=\"{logo}{c['name']}.png\",{c['name']}\n{c['url']}\n")
        with open(txt_p, 'w', encoding='utf-8') as ft:
            ft.write(f"# Updated: {update_ts}\n# Duration: {duration}\n")
            for c in valid_list:
                ft.write(f"{c['name']},{c['url']}\n")
    except Exception as e:
        status["logs"].append(f"âš ï¸ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

    status["running"] = False

    # è§¦å‘åŒ…å«æ­¤è®¢é˜…çš„èšåˆä»»åŠ¡è‡ªåŠ¨æ›´æ–°
    with db_session() as session:
        for agg in session.query(Aggregate).filter(Aggregate.subscription_ids.contains(sub_id)).all():
            threading.Thread(target=run_aggregate, args=(agg.id,), kwargs={"auto": True}).start()

# ---------- èšåˆä»»åŠ¡ï¼ˆä¿ç•™æ‰€æœ‰URLï¼Œå¹¶å°†æœªåœ¨demo.txtä¸­çš„é¢‘é“å½’å…¥â€œå…¶ä»–é¢‘é“â€ï¼‰----------
def run_aggregate(agg_id, auto=False):
    if aggregates_status.get(agg_id, {}).get("running"):
        return
    aggregates_status[agg_id] = {"running": True, "logs": []}
    
    def log(msg):
        ts = get_now()
        aggregates_status[agg_id]["logs"].append(f"{ts} - {msg}")
    
    log(f"ğŸš€ èšåˆä»»åŠ¡å¼€å§‹ (è‡ªåŠ¨: {auto})")
    
    with db_session() as session:
        agg = session.get(Aggregate, agg_id)
        if not agg or not agg.enabled:
            log("âŒ èšåˆé…ç½®ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            aggregates_status[agg_id]["running"] = False
            return

        log(f"ğŸ“‹ èšåˆåç§°: {agg.name}")
        log(f"ğŸ“¦ åŒ…å«è®¢é˜…: {', '.join(agg.subscription_ids or [])}")

        # è·å–æ‰€æœ‰æ¢æµ‹ç»“æœ
        results = []
        for sid in agg.subscription_ids or []:
            sub_results = session.query(ProbeResult).filter(ProbeResult.sub_id == sid).all()
            results.extend(sub_results)
        log(f"ğŸ“Š ä»æ•°æ®åº“è¯»å– {len(results)} æ¡åŸå§‹æ¢æµ‹ç»“æœ")

        # æŒ‰æ ‡å‡†åç§°åˆ†ç»„ï¼Œæ¯ä¸ªæ ‡å‡†åç§°å¯¹åº”ä¸€ä¸ªåˆ—è¡¨
        channel_map = {}
        for r in results:
            std_name, matched = match_channel_name(r.channel_name)
            if std_name not in channel_map:
                channel_map[std_name] = []
            channel_map[std_name].append({
                "name": std_name,
                "url": r.url,
                "score": r.score,
                "res_tag": r.res_tag
            })

        # å¯¹æ¯ä¸ªæ ‡å‡†åç§°ä¸‹çš„URLæŒ‰è¯„åˆ†é™åºæ’åº
        for name in channel_map:
            channel_map[name].sort(key=lambda x: x['score'], reverse=True)

        log(f"ğŸ“Š èšåˆåå¾—åˆ° {len(channel_map)} ä¸ªæ ‡å‡†é¢‘é“ï¼Œå…±è®¡ {sum(len(v) for v in channel_map.values())} ä¸ªURL")

    # è¯»å– demo.txt è·å–é¡ºåºå’Œåˆ†ç»„ä¿¡æ¯
    ordered_names = []
    group_map = {}
    if os.path.exists(DEMO_FILE):
        current_group = "æœªåˆ†ç»„"
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if ',#genre#' in line:
                    current_group = line.split(',')[0].strip()
                    log(f"ğŸ“‚ è¯†åˆ«åˆ†ç»„: {current_group}")
                else:
                    name = line
                    ordered_names.append(name)
                    group_map[name] = current_group
        log(f"ğŸ“‹ ä» demo.txt åŠ è½½äº† {len(ordered_names)} ä¸ªé¢‘é“é¡ºåº")
    else:
        # å¦‚æœæ²¡æœ‰demo.txtï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰æ ‡å‡†åç§°æŒ‰å­—æ¯æ’åº
        ordered_names = sorted(channel_map.keys())
        log(f"ğŸ“‹ æœªæ‰¾åˆ° demo.txtï¼Œä½¿ç”¨å­—æ¯é¡ºåº")

    # æŒ‰é¡ºåºç”Ÿæˆæœ€ç»ˆåˆ—è¡¨ï¼ˆå…ˆå¤„ç† demo.txt ä¸­çš„é¢‘é“ï¼‰
    final_list = []
    for name in ordered_names:
        if name in channel_map:
            for item in channel_map[name]:
                item["group"] = group_map.get(name, "æœªåˆ†ç»„")
                final_list.append(item)

    # å¤„ç†ä¸åœ¨ demo.txt ä¸­çš„é¢‘é“ï¼Œå½’å…¥â€œå…¶ä»–é¢‘é“â€åˆ†ç»„
    remaining_names = set(channel_map.keys()) - set(ordered_names)
    if remaining_names:
        log(f"ğŸ“¦ å‘ç° {len(remaining_names)} ä¸ªé¢‘é“ä¸åœ¨ demo.txt ä¸­ï¼Œå°†å½’å…¥â€œå…¶ä»–é¢‘é“â€åˆ†ç»„")
        for name in sorted(remaining_names):  # æŒ‰å­—æ¯æ’åº
            for item in channel_map[name]:
                item["group"] = "å…¶ä»–é¢‘é“"
                final_list.append(item)

    log(f"âœ… æœ€ç»ˆç”Ÿæˆ {len(final_list)} ä¸ªæœ‰æ•ˆé“¾æ¥")

    # ç¡®å®šä½¿ç”¨çš„ EPG URL
    config = load_config()
    epg_url = config.get("settings", {}).get("epg_url", "")
    epg_agg_id = agg.epg_aggregate_id
    if epg_agg_id:
        with db_session() as session:
            epg_agg = session.get(EPGAggregate, epg_agg_id)
            if epg_agg:
                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé¿å… request ä¸Šä¸‹æ–‡é—®é¢˜
                epg_url = f"/epg/{epg_agg_id}.xml"
                log(f"ğŸ“º ä½¿ç”¨ EPG èšåˆ: {epg_agg.name} -> {epg_url}")
            else:
                log(f"âš ï¸ æŒ‡å®šçš„ EPG èšåˆä¸å­˜åœ¨ï¼Œä½¿ç”¨å…¨å±€ EPG")
    else:
        log(f"ğŸ“º ä½¿ç”¨å…¨å±€ EPG: {epg_url}")

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    update_ts = get_now()
    logo_base = config.get("settings", {}).get("logo_base", "")
    m3u_path = os.path.join(OUTPUT_DIR, f"aggregate_{agg_id}.m3u")
    txt_path = os.path.join(OUTPUT_DIR, f"aggregate_{agg_id}.txt")
    
    with open(m3u_path, 'w', encoding='utf-8') as fm:
        fm.write(f"#EXTM3U x-tvg-url=\"{epg_url}\"\n# Updated: {update_ts}\n")
        for c in final_list:
            tvg_name = c['name']
            tvg_logo = f"{logo_base}{tvg_name}.png"
            group_title = c.get('group', 'æœªåˆ†ç»„')
            fm.write(f"#EXTINF:-1 tvg-name=\"{tvg_name}\" tvg-logo=\"{tvg_logo}\" group-title=\"{group_title}\",{tvg_name}\n")
            fm.write(f"{c['url']}\n")

    with open(txt_path, 'w', encoding='utf-8') as ft:
        ft.write(f"# Updated: {update_ts}\n")
        for c in final_list:
            ft.write(f"{c['name']},{c['url']}\n")

    log(f"ğŸ’¾ æ–‡ä»¶å·²å†™å…¥: {m3u_path}, {txt_path}")

    # æ›´æ–°èšåˆæœ€åæ›´æ–°æ—¶é—´
    with db_session() as session:
        agg = session.get(Aggregate, agg_id)
        if agg:
            agg.last_update = datetime.datetime.now()
            session.commit()

    log(f"ğŸ èšåˆä»»åŠ¡å®Œæˆ")
    aggregates_status[agg_id]["running"] = False

# ---------- EPG èšåˆï¼ˆå¢å¼ºç‰ˆï¼‰----------
def run_epg_aggregate(epg_agg_id, auto=False):
    try:
        if epg_aggregates_status.get(epg_agg_id, {}).get("running"):
            return
        epg_aggregates_status[epg_agg_id] = {"running": True, "logs": []}
        
        def log(msg):
            ts = get_now()
            epg_aggregates_status[epg_agg_id]["logs"].append(f"{ts} - {msg}")
        
        log(f"ğŸ“º EPG èšåˆä»»åŠ¡å¼€å§‹ (è‡ªåŠ¨: {auto})")
        
        # ä»æ•°æ®åº“è·å– EPG èšåˆé…ç½®
        with db_session() as session:
            epg_agg = session.get(EPGAggregate, epg_agg_id)
            if not epg_agg or not epg_agg.enabled:
                log("âŒ EPG èšåˆé…ç½®ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
                epg_aggregates_status[epg_agg_id]["running"] = False
                return

            log(f"ğŸ“‹ EPG èšåˆåç§°: {epg_agg.name}")
            log(f"ğŸ”— æºåˆ—è¡¨: {', '.join(epg_agg.sources)}")
            cache_days = epg_agg.cache_days or 3
            log(f"ğŸ“… ç¼“å­˜å¤©æ•°: {cache_days}")

        today = datetime.date.today()
        date_list = [today + datetime.timedelta(days=i) for i in range(-1, cache_days)]
        date_strs = [d.strftime('%Y%m%d') for d in date_list]
        log(f"ğŸ“… éœ€è¦åŒ…å«çš„æ—¥æœŸ: {', '.join(date_strs)}")

        programmes = {}
        channels_dict = {}

        # ä¸‹è½½å¹¶è§£ææ¯ä¸ªæº
        for idx, source_url in enumerate(epg_agg.sources):
            log(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æº {idx+1}: {source_url}")
            try:
                resp = requests.get(source_url, timeout=30)
                if resp.status_code != 200:
                    log(f"âš ï¸ æº {source_url} è¿”å›çŠ¶æ€ç  {resp.status_code}ï¼Œè·³è¿‡")
                    continue
                content = resp.content

                # å¤„ç†å¯èƒ½ä¸º gzip å‹ç¼©çš„å†…å®¹
                is_gz = source_url.endswith('.gz')
                if is_gz:
                    try:
                        buf = BytesIO(content)
                        with gzip.GzipFile(fileobj=buf) as gz_file:
                            content = gz_file.read()
                        log(f"ğŸ“¦ æ£€æµ‹åˆ° gzip å‹ç¼©ï¼Œå·²è§£å‹")
                    except Exception as e:
                        log(f"âš ï¸ è§£å‹å¤±è´¥: {str(e)}ï¼Œå°†ä½œä¸ºæ™®é€š XML å°è¯•è§£æ")
                        content = resp.content  # æ¢å¤åŸå§‹å†…å®¹

                try:
                    tree = ET.parse(BytesIO(content))
                    root = tree.getroot()
                except Exception as e:
                    log(f"âŒ è§£æ XML å¤±è´¥: {str(e)}")
                    continue

                channels_added = 0
                for channel in root.findall('channel'):
                    ch_id = channel.get('id')
                    if ch_id:
                        std_name, matched = match_channel_name(ch_id)
                        if ch_id not in channels_dict:
                            channels_dict[ch_id] = (channel, std_name if matched else None)
                            channels_added += 1
                if channels_added > 0:
                    log(f"ğŸ“º æº {idx+1} æ·»åŠ äº† {channels_added} ä¸ªé¢‘é“")

                count = 0
                for prog in root.findall('programme'):
                    start = prog.get('start')
                    channel = prog.get('channel')
                    title_elem = prog.find('title')
                    title = title_elem.text if title_elem is not None else ''
                    if start and len(start) >= 8:
                        prog_date = start[:8]
                        if prog_date in date_strs:
                            key = (channel, start, title)
                            if key not in programmes:
                                programmes[key] = prog
                                count += 1
                log(f"â• æº {idx+1} æ·»åŠ äº† {count} ä¸ªèŠ‚ç›®")
            except Exception as e:
                log(f"âŒ ä¸‹è½½æº {source_url} å¤±è´¥: {str(e)}")

        log(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(channels_dict)} ä¸ªé¢‘é“ï¼Œ{len(programmes)} ä¸ªèŠ‚ç›®")

        new_root = ET.Element('tv')
        for ch_id, (ch_elem, std_name) in channels_dict.items():
            new_ch = copy.deepcopy(ch_elem)
            if std_name:
                dn = ET.SubElement(new_ch, 'display-name')
                dn.text = std_name
            new_root.append(new_ch)
        for prog in programmes.values():
            new_root.append(prog)

        update_ts = get_now()
        xml_path = os.path.join(OUTPUT_DIR, f"epg_{epg_agg_id}.xml")
        tree = ET.ElementTree(new_root)
        tree.write(xml_path, encoding='utf-8', xml_declaration=True)
        log(f"ğŸ’¾ XML å·²ä¿å­˜: {xml_path}")

        with db_session() as session:
            epg = session.get(EPGAggregate, epg_agg_id)
            if epg:
                epg.last_update = datetime.datetime.now()
                session.commit()

        epg_status = {
            "update_time": update_ts,
            "total": len(programmes),
            "channels": len(channels_dict),
            "sources": epg_agg.sources,
            "files": {"xml": f"/epg/{epg_agg_id}.xml"}
        }
        status_path = os.path.join(OUTPUT_DIR, f"epg_{epg_agg_id}_status.json")
        with open(status_path, 'w', encoding='utf-8') as f:
            json.dump(epg_status, f, ensure_ascii=False)

        log(f"ğŸ EPG èšåˆä»»åŠ¡å®Œæˆ")
        epg_aggregates_status[epg_agg_id]["running"] = False
    except Exception as e:
        epg_aggregates_status[epg_agg_id]["running"] = False
        log(f"âŒ èšåˆä»»åŠ¡å¼‚å¸¸: {str(e)}")

# ---------- è®¡åˆ’ä»»åŠ¡è°ƒåº¦ ----------
def clear_sub_jobs(sub_id):
    for job in scheduler.get_jobs():
        if job.id.startswith(sub_id):
            scheduler.remove_job(job.id)

def schedule_subscription(sub):
    sub_id = sub.id
    clear_sub_jobs(sub_id)
    if not sub.enabled:
        return
    mode = sub.schedule_mode
    if mode == "none":
        return
    elif mode == "fixed":
        times = (sub.fixed_times or "").split(",")
        for t in times:
            t = t.strip()
            if not t:
                continue
            try:
                hour, minute = map(int, t.split(':'))
                job_id = f"{sub_id}_fixed_{hour:02d}{minute:02d}"
                scheduler.add_job(
                    func=run_task,
                    args=[sub_id],
                    trigger='cron',
                    hour=hour,
                    minute=minute,
                    id=job_id,
                    replace_existing=True
                )
            except Exception as e:
                app.logger.error(f"è°ƒåº¦ fixed ä»»åŠ¡å¤±è´¥ {sub_id} {t}: {e}")
    elif mode == "interval":
        hours = sub.interval_hours or 1
        job_id = f"{sub_id}_interval"
        scheduler.add_job(
            func=run_task,
            args=[sub_id],
            trigger='interval',
            hours=hours,
            id=job_id,
            replace_existing=True
        )

def reschedule_all():
    with db_session() as session:
        for sub in session.query(Subscription).all():
            schedule_subscription(sub)

def clear_epg_jobs(epg_agg_id):
    for job in scheduler.get_jobs():
        if job.id.startswith(f"epg_{epg_agg_id}"):
            scheduler.remove_job(job.id)

def schedule_epg_aggregation(epg_agg):
    epg_id = epg_agg.id
    clear_epg_jobs(epg_id)
    if not epg_agg.enabled:
        return
    interval = epg_agg.update_interval or 24
    job_id = f"epg_{epg_id}_interval"
    scheduler.add_job(
        func=run_epg_aggregate,
        args=[epg_id],
        kwargs={"auto": True},
        trigger='interval',
        hours=interval,
        id=job_id,
        replace_existing=True
    )

def reschedule_epg_all():
    with db_session() as session:
        for epg_agg in session.query(EPGAggregate).all():
            schedule_epg_aggregation(epg_agg)

# ---------- Flask è·¯ç”± ----------
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/m3u_aggregate')
def m3u_aggregate_page():
    return render_template('m3u_aggregate.html')

@app.route('/epg_aggregate')
def epg_aggregate_page():
    return render_template('epg_aggregate.html')

@app.route('/pending')
def pending_page():
    return render_template('pending.html')

@app.route('/api/sys_info')
def sys_info():
    try:
        gpu = 0
        if os.path.exists("/sys/class/drm/card0/device/gpu_busy_percent"):
            with open("/sys/class/drm/card0/device/gpu_busy_percent", 'r') as f:
                gpu = int(f.read().strip())
        return jsonify({
            "cpu": psutil.cpu_percent(),
            "ram": psutil.virtual_memory().percent,
            "gpu": gpu,
            "gpu_active": any(s.get("running") for s in subs_status.values())
        })
    except:
        return jsonify({"cpu": 0, "ram": 0, "gpu": 0})

@app.route('/api/network_test')
def network_test():
    res = {"v4": {"status": False, "ip": ""}, "v6": {"status": False, "ip": ""}}
    
    ipv4_services = [
        "https://api4.ipify.org?format=json",
        "https://api.ip.sb/ip?format=json",
        "https://ipv4.icanhazip.com/"
    ]
    for service in ipv4_services:
        try:
            if service.endswith('.com/'):
                r = requests.get(service, timeout=8)
                ip = r.text.strip()
                if ip:
                    res["v4"] = {"status": True, "ip": ip}
                    break
            else:
                r = requests.get(service, timeout=8).json()
                ip = r.get('ip') or r.get('IPv4')
                if ip:
                    res["v4"] = {"status": True, "ip": ip}
                    break
        except:
            continue

    try:
        r6 = requests.get("https://api6.ipify.org?format=json", timeout=8).json()
        res["v6"] = {"status": True, "ip": r6['ip']}
    except:
        pass

    return jsonify(res)

@app.route('/api/subs', methods=['GET', 'POST'])
def handle_subs():
    if request.method == 'POST':
        data = request.json
        with db_session() as session:
            if not data.get("id"):
                data["id"] = str(uuid.uuid4())[:8]
                sub = Subscription(**data)
                session.add(sub)
            else:
                sub = session.get(Subscription, data["id"])
                if sub:
                    for k, v in data.items():
                        setattr(sub, k, v)
            session.commit()
        return jsonify({"status": "ok"})
    else:
        with db_session() as session:
            subs = [{
                "id": s.id, "name": s.name, "url": s.url, "threads": s.threads,
                "enabled": s.enabled, "schedule_mode": s.schedule_mode,
                "fixed_times": s.fixed_times, "interval_hours": s.interval_hours,
                "res_filter": s.res_filter
            } for s in session.query(Subscription).all()]
            settings = {s.key: s.value for s in session.query(Setting).all()}
            return jsonify({"subs": subs, "settings": settings})

@app.route('/api/status/<sub_id>')
def get_status(sub_id):
    limit = request.args.get('limit', default=150, type=int)
    if sub_id in subs_status:
        s = subs_status[sub_id]
        return jsonify({
            "running": s["running"],
            "logs": s["logs"][-limit:],
            "total": s["total"],
            "current": s["current"],
            "success": s["success"],
            "banned_count": len(s.get("blacklisted_hosts", [])),
            "analytics": s["analytics"]
        })
    archive_path = os.path.join(OUTPUT_DIR, f"last_status_{sub_id}.json")
    if os.path.exists(archive_path):
        with open(archive_path, 'r', encoding='utf-8') as f:
            d = json.load(f)
            return jsonify({
                "running": False,
                "logs": d["logs"][-limit:],
                "total": d["stats"]["total"],
                "current": d["stats"]["current"],
                "success": d["stats"]["success"],
                "banned_count": d["stats"]["banned"],
                "analytics": d["analytics"]
            })
    return jsonify({"running": False, "logs": [], "total": 0, "current": 0, "success": 0, "banned_count": 0, "analytics": {}})

@app.route('/api/start/<sub_id>')
def start_api(sub_id):
    threading.Thread(target=run_task, args=(sub_id,)).start()
    return jsonify({"status": "ok"})

@app.route('/api/stop/<sub_id>')
def stop_api(sub_id):
    if sub_id in subs_status:
        subs_status[sub_id]["stop_requested"] = True
    return jsonify({"status": "ok"})

@app.route('/api/settings', methods=['POST'])
def save_settings():
    data = request.json
    with db_session() as session:
        for key, value in data.items():
            setting = session.get(Setting, key)
            if setting:
                setting.value = str(value)
            else:
                session.add(Setting(key=key, value=str(value)))
        session.commit()
    return jsonify({"status": "ok"})

@app.route('/api/hw_test')
def hw_test():
    try:
        r = subprocess.run(['vainfo'], capture_output=True, text=True, timeout=5)
        out = r.stdout + r.stderr
        ready = "va_openDriver() returns 0" in out
        codecs = []
        mapping = {"H264": "H264", "HEVC (H.265)": "HEVC|H265", "VP9": "VP9", "MPEG2": "MPEG2"}
        for k, v in mapping.items():
            if any(x in out.upper() for x in v.split('|')):
                codecs.append(k)
        return jsonify({
            "status": "success" if ready else "error",
            "message": "âœ… GPUåŠ é€Ÿå°±ç»ª" if ready else "âŒ é©±åŠ¨å¼‚å¸¸",
            "codecs": codecs,
            "raw": out
        })
    except Exception as e:
        return jsonify({"status": "error", "raw": str(e)})

@app.route('/api/subs/delete/<sub_id>')
def delete_sub(sub_id):
    with db_session() as session:
        sub = session.get(Subscription, sub_id)
        if sub:
            session.delete(sub)
            session.commit()
    clear_sub_jobs(sub_id)
    return jsonify({"status": "ok"})

@app.route('/sub/<sub_id>.<ext>')
def get_sub_file(sub_id, ext):
    return send_from_directory(OUTPUT_DIR, f"{sub_id}.{ext}")

# ---------- èšåˆç›¸å…³ API ----------
@app.route('/api/aggregates', methods=['GET', 'POST'])
def api_aggregates():
    if request.method == 'POST':
        data = request.json
        with db_session() as session:
            if not data.get("id"):
                data["id"] = str(uuid.uuid4())[:8]
                agg = Aggregate(**data)
                session.add(agg)
            else:
                agg = session.get(Aggregate, data["id"])
                if agg:
                    for k, v in data.items():
                        setattr(agg, k, v)
            session.commit()
        return jsonify({"status": "ok"})
    else:
        with db_session() as session:
            result = []
            for agg in session.query(Aggregate).all():
                agg_dict = {
                    "id": agg.id,
                    "name": agg.name,
                    "subscription_ids": agg.subscription_ids,
                    "strategy": agg.strategy,
                    "enabled": agg.enabled,
                    "epg_aggregate_id": agg.epg_aggregate_id,
                    "last_update": agg.last_update.strftime('%Y-%m-%d %H:%M:%S') if agg.last_update else "ä»æœª"
                }
                result.append(agg_dict)
            return jsonify(result)

@app.route('/api/aggregate/run/<agg_id>')
def run_aggregate_api(agg_id):
    threading.Thread(target=run_aggregate, args=(agg_id,), kwargs={"auto": False}).start()
    return jsonify({"status": "ok"})

@app.route('/api/aggregate/log/<agg_id>')
def get_aggregate_log(agg_id):
    logs = aggregates_status.get(agg_id, {}).get("logs", [])
    return jsonify({"logs": logs})

@app.route('/api/aggregate/delete/<agg_id>')
def delete_aggregate(agg_id):
    with db_session() as session:
        agg = session.get(Aggregate, agg_id)
        if agg:
            session.delete(agg)
            session.commit()
    return jsonify({"status": "ok"})

@app.route('/aggregate/<agg_id>.<ext>')
def get_aggregate_file(agg_id, ext):
    return send_from_directory(OUTPUT_DIR, f"aggregate_{agg_id}.{ext}")

# ---------- EPG èšåˆç›¸å…³ API ----------
@app.route('/api/epg_aggregates', methods=['GET', 'POST'])
def api_epg_aggregates():
    if request.method == 'POST':
        data = request.json
        with db_session() as session:
            if not data.get("id"):
                data["id"] = str(uuid.uuid4())[:8]
                epg = EPGAggregate(**data)
                session.add(epg)
            else:
                epg = session.get(EPGAggregate, data["id"])
                if epg:
                    for k, v in data.items():
                        setattr(epg, k, v)
            session.commit()
        return jsonify({"status": "ok"})
    else:
        with db_session() as session:
            result = []
            for epg in session.query(EPGAggregate).all():
                epg_dict = {
                    "id": epg.id,
                    "name": epg.name,
                    "sources": epg.sources,
                    "cache_days": epg.cache_days,
                    "update_interval": epg.update_interval,
                    "enabled": epg.enabled,
                    "last_update": epg.last_update.strftime('%Y-%m-%d %H:%M:%S') if epg.last_update else "ä»æœª"
                }
                result.append(epg_dict)
            return jsonify(result)

@app.route('/api/epg_aggregate/run/<epg_id>')
def run_epg_aggregate_api(epg_id):
    threading.Thread(target=run_epg_aggregate, args=(epg_id,), kwargs={"auto": False}).start()
    return jsonify({"status": "ok"})

@app.route('/api/epg_aggregate/log/<epg_id>')
def get_epg_aggregate_log(epg_id):
    logs = epg_aggregates_status.get(epg_id, {}).get("logs", [])
    return jsonify({"logs": logs})

@app.route('/api/epg_aggregate/delete/<epg_id>')
def delete_epg_aggregate(epg_id):
    with db_session() as session:
        epg = session.get(EPGAggregate, epg_id)
        if epg:
            session.delete(epg)
            session.commit()
    return jsonify({"status": "ok"})

@app.route('/epg/<epg_id>.xml')
def get_epg_xml(epg_id):
    filename = f"epg_{epg_id}.xml"
    return send_from_directory(OUTPUT_DIR, filename)

# ---------- EPG é¢‘é“æ£€æŸ¥ API ----------
@app.route('/api/epg_check/<epg_id>')
def epg_check(epg_id):
    channel = request.args.get('channel', '').strip()
    if not channel:
        return jsonify({"error": "é¢‘é“åç§°ä¸èƒ½ä¸ºç©º"}), 400
    xml_path = os.path.join(OUTPUT_DIR, f"epg_{epg_id}.xml")
    if not os.path.exists(xml_path):
        return jsonify({"exists": False, "message": "EPG æ–‡ä»¶ä¸å­˜åœ¨"})
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        channels = []
        for ch in root.findall('channel'):
            ch_id = ch.get('id', '')
            if channel.lower() in ch_id.lower():
                channels.append(ch_id)
            for dn in ch.findall('display-name'):
                if channel.lower() in (dn.text or '').lower():
                    channels.append(ch_id)
        programmes = []
        for prog in root.findall('programme'):
            prog_ch = prog.get('channel', '')
            if channel.lower() in prog_ch.lower():
                programmes.append({
                    "channel": prog_ch,
                    "start": prog.get('start'),
                    "title": prog.findtext('title', '')
                })
        return jsonify({
            "channel_exists": len(channels) > 0,
            "programme_count": len(programmes),
            "matched_channels": list(set(channels)),
            "matched_programmes_sample": programmes[:5]
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ---------- å¾…å¤„ç†é¢‘é“ API ----------
@app.route('/api/groups')
def get_groups():
    groups = []
    if os.path.exists(DEMO_FILE):
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and ',#genre#' in line:
                    group_name = line.split(',')[0].strip()
                    groups.append(group_name)
    return jsonify(groups)

@app.route('/api/pending', methods=['GET'])
def get_pending():
    with db_session() as session:
        pendings = session.query(PendingChannel).order_by(PendingChannel.count.desc()).all()
        return jsonify([{
            "name": p.raw_name,
            "count": p.count,
            "first_seen": p.first_seen.strftime('%Y-%m-%d %H:%M:%S'),
            "sub_ids": p.sub_ids
        } for p in pendings])

@app.route('/api/channel_names')
def get_channel_names():
    """è¿”å›æ‰€æœ‰å·²çŸ¥çš„æ ‡å‡†åï¼ˆç”¨äºåˆ«åè¾“å…¥å»ºè®®ï¼‰"""
    names = set()
    # ä» alias.txt è·å–ä¸»å
    if os.path.exists(ALIAS_FILE):
        with open(ALIAS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                parts = line.split(',')
                if parts:
                    names.add(parts[0].strip())
    # ä» demo.txt è·å–é¢‘é“åï¼ˆéåˆ†ç»„è¡Œï¼‰
    if os.path.exists(DEMO_FILE):
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if ',#genre#' not in line:
                    names.add(line)
    return jsonify(sorted(names))

@app.route('/api/alias/<main_name>')
def get_aliases(main_name):
    """è¿”å›æŒ‡å®šæ ‡å‡†ååœ¨ alias.txt ä¸­çš„æ‰€æœ‰åˆ«åï¼ˆä¸åŒ…æ‹¬ä¸»åæœ¬èº«ï¼‰"""
    aliases = []
    if os.path.exists(ALIAS_FILE):
        with open(ALIAS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                parts = line.split(',')
                if parts and parts[0].strip() == main_name:
                    aliases = [a.strip() for a in parts[1:] if a.strip()]
                    break
    return jsonify(aliases)

@app.route('/api/pending/ignore', methods=['POST'])
def ignore_pending():
    data = request.json
    name = data.get('name')
    if not name:
        return jsonify({"error": "ç¼ºå°‘é¢‘é“å"}), 400
    with db_session() as session:
        pc = session.query(PendingChannel).filter_by(raw_name=name).first()
        if pc:
            session.delete(pc)
            session.commit()
    return jsonify({"status": "ok"})

@app.route('/api/pending/set_alias', methods=['POST'])
def set_alias():
    data = request.json
    raw_name = data.get('raw_name')
    main_name = data.get('main_name')
    aliases = data.get('aliases', [])
    if not raw_name or not main_name:
        return jsonify({"error": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400
    all_aliases = list(set([raw_name] + aliases))
    append_alias(main_name, all_aliases)
    with db_session() as session:
        pc = session.query(PendingChannel).filter_by(raw_name=raw_name).first()
        if pc:
            session.delete(pc)
            session.commit()
    return jsonify({"status": "ok"})

@app.route('/api/pending/set_group', methods=['POST'])
def set_group():
    data = request.json
    channel_name = data.get('channel_name')
    group_name = data.get('group_name')
    if not channel_name or not group_name:
        return jsonify({"error": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400
    append_to_demo(channel_name, group_name)
    with db_session() as session:
        pc = session.query(PendingChannel).filter_by(raw_name=channel_name).first()
        if pc:
            session.delete(pc)
            session.commit()
    return jsonify({"status": "ok"})

# ---------- å¯åŠ¨æ—¶åˆå§‹åŒ–è°ƒåº¦ ----------
with app.app_context():
    reschedule_all()
    reschedule_epg_all()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5123)
